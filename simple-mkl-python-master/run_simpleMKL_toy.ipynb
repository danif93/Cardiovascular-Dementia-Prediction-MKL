{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, \n",
    "#                     n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, \n",
    "#                     hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "ds, label = datasets.make_classification(n_samples=300, n_features=30, n_informative=10, n_redundant=0, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(label==0)\n",
    "label[np.where(label==0)]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X, y, n_samples):\n",
    "    \"\"\"Extract n_samples from X and y.\"\"\"\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    \n",
    "    return next(StratifiedShuffleSplit(n_splits=1, test_size=n_samples).split(X, y))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx, ts_idx = random_sampling(ds, label, n_samples=0.25)\n",
    "\n",
    "\n",
    "ds1 = np.hstack([ds[:,:5], ds[:,10:15]])\n",
    "ds1_tr = ds1[tr_idx]\n",
    "ds1_ts = ds1[ts_idx]\n",
    "\n",
    "ds2 = np.hstack([ds[:,5:10], ds[:,15:20]])\n",
    "ds2_tr = ds2[tr_idx]\n",
    "ds2_ts = ds2[ts_idx]\n",
    "\n",
    "ds3 = ds[:,20:]\n",
    "ds3_tr = ds3[tr_idx]\n",
    "ds3_ts = ds3[ts_idx]\n",
    "\n",
    "l_tr = label[tr_idx]\n",
    "l_ts = label[ts_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algo1\n",
    "import numpy as np\n",
    "import kernel_helpers as k_helpers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = ds[tr_idx]  # first 200 examples\n",
    "train_labels = label[tr_idx]  # first 200 labels\n",
    "\n",
    "test_data = ds[ts_idx]  # example 201 onwards\n",
    "test_labels = label[ts_idx]  # label 201 onwards\n",
    "\n",
    "\n",
    "# parameters for the kernels we'll use\n",
    "gamma = 1.0/d\n",
    "intercept = 0\n",
    "\n",
    "kernel_functions = [\n",
    "    k_helpers.linear_kernel,\n",
    "    k_helpers.create_rbf_kernel(gamma),\n",
    "    k_helpers.create_poly_kernel(2, gamma),\n",
    "    k_helpers.create_poly_kernel(3, gamma),\n",
    "    k_helpers.create_poly_kernel(4, gamma),\n",
    "    k_helpers.create_sigmoid_kernel(gamma),\n",
    "]\n",
    "\n",
    "weights = algo1.find_kernel_weights(train_data, train_labels, kernel_functions)\n",
    "print ('Final weights for each kernel are: {}'.format(weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
