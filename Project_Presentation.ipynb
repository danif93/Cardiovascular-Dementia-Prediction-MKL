{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCB Final Project - a.y. 17/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Fiaschi Lorenzo, Franco Danilo</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol> <b>TODO</b>\n",
    "    <li><strike>problem presentation (dataset, objectives)</strike></li>\n",
    "    <li><strike>preprocessing</strike></li>\n",
    "    <li><strike>mkl procedure overview</strike></li>\n",
    "    <li><strike>chosen model description (alignment model, cortes approach)</strike></li>\n",
    "    <li>algorithm pipeline (with a major point in the cross-validation decisions, how it is shuffled...)</li>\n",
    "    <li><strike>approach settings</strike></li>\n",
    "    <li>performances over toy (with description on how it is generated</li>\n",
    "    <li>integration needed? comparison with single dataset model</li>\n",
    "    <li>performances over real dataset</li>\n",
    "    <li>integration needed? comparison with single dataset model</li>\n",
    "    <li>what we can learn from the best configuration (sparsity)</li>\n",
    "    <li>kernel integration on the only meaningful dataset (clinical)</li>\n",
    "    <li>overview on the regression approach</li>\n",
    "    <li>regression performances with data integration</li>\n",
    "    <li>single kernel approach after regression results</li>\n",
    "    <li>final comments over the results</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Problem\n",
    "The dataset consists of 2741 aligned patients affected by Diabetes; it is structured in 3 differents tables:\n",
    "<ol>\n",
    "    <li>Genetic Features, 347 - Related to the SNP of 344 genes + 3 composite genes scores [blood pressure, alzheimer, CVD]\n",
    "    </li><li>Retina's Features, 157 - Engineered features extracted from patients' retinas</li>\n",
    "    <li>Clinical Features, 15 - General patients' information</li>\n",
    "</ol>\n",
    "The objective of the study is to find a model that is able to predict whether a patient will incurr in either cardiovascular failures or in episodes of dementia.<br>\n",
    "In the positive case, it is also aked to predict at which age this episodes will occurr.<br>\n",
    "In order to learn such model, the output dataset has been provided; it is composed by two information, both for the occurrence of a cardiovascular failure and for the dementia episode: whether the episode has happened or not and at what age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing\n",
    "In order to be fed, the tables have been cleaned as follows:\n",
    "<ul>\n",
    "    <li>\n",
    "        <b><i>NaN Filling</i></b>: \n",
    "        <ul>\n",
    "            <li>\n",
    "                Clinical DS: \n",
    "                <ul>\n",
    "                    <li>\n",
    "                        [\"therapy\",\"gender\",\"precedent CVD\", \"smoker\"] $\\rightarrow$ most frequent;\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        Others $\\rightarrow$ mean.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            <li>\n",
    "                Genetic DS: \n",
    "                <ul>\n",
    "                    <li>\n",
    "                        [composite gene scores] $\\rightarrow$ mean;\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        Others $\\rightarrow$ min.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            <li>\n",
    "                Vampire DS: \n",
    "                <ul>\n",
    "                    <li>\n",
    "                        All $\\rightarrow$ mean.\n",
    "                    </li>\n",
    "                </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b><i>One-Hot Encoding</i></b>:\n",
    "        <ul>\n",
    "            <li>Clinical DS:\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        \"therapy\" and \"Apoe4Presence\".\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b><i>Boolean: from binary to symmetric</i></b>: \n",
    "        <ul>\n",
    "            <li>Clinical DS:\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        [\"gender\",\"precedent CVD\", \"smoker\"] $\\rightarrow$ from [0,1] to [-1,1].\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Outputs:\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        [\"cvd_fail\",\"dement_fail\"] $\\rightarrow$ from [0,1] to [-1,1].\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multiple Kernel Learning\n",
    "<b>Multiple Kernel Learning</b> refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm.<br>\n",
    "Reasons to use multiple kernel learning include:\n",
    "<ul>\n",
    "    <li>the ability to select for an optimal kernel and parameters from a larger set of kernels, reducing bias due to kernel selection while allowing for more automated machine learning methods;\n",
    "    </li>\n",
    "    <li>combining data from different sources (e.g. sound and images from a video) that have different notions of similarity and thus require different kernels;\n",
    "    </li>\n",
    "</ul>\n",
    "<img src='others/mkl.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Model\n",
    "<ul>\n",
    "    <li><b><i>Target Function</i></b> $\\rightarrow$ Similarity-based function; it uses a similarity metric between the combined kernel matrix and an optimum kernel matrix calculated from the training data, in order to select the combination function parameters that maximize the similarity. The similarity between two kernel matrices can be calculated using kernel alignment, Euclidean distance, Kullback-Leibler (KL) divergence, or any other similarity measure.\n",
    "    </li>\n",
    "    <li><b><i>Training Method</i></b> $\\rightarrow$ One-step method; it calculates both the combination function parameters and the parameters of the combined base learner in a single pass. One can use a sequential approach or a simultaneous approach. In the sequential approach, the combination function parameters are determined first, and then a kernel-based learner is trained using the combined kernel. In the simultaneous approach, both set of parameters are learned together.\n",
    "    </li>\n",
    "</ul>\n",
    "The chosen algorithm is called <i>Centered-kernel alignment</i> [1] and its main purpose is to compute a set of $P$ kernels $K_i$, $i \\, = \\, 1...P$ from the datasets and to use a linear combination of them to approximate an ideal kernel. Then the obtained approximated kernel can be used for both classification and regression purposes. The functional is:<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\max\\limits_{\\eta \\in \\mathcal{M}} CA(K_{\\eta}, IK)\n",
    "\\label{eq:problem}\n",
    "\\end{equation}\n",
    "\n",
    "where $IK = y^T y$ is the ideal kernel, $K_{\\eta} = \\sum\\limits_{i=1}^P K_i \\eta_i$ is its approximation and $\\mathcal{M}=\\{\\eta : ||\\eta||_2 = 1\\}$ imposing $\\eta$ being a unit norm vector. In turn, $CA(K_1, K_2)$ is the defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "CA(K_1^c, K_2^c) = \\frac{\\langle K_1^c, K_2^c \\rangle_F}{\\sqrt{\\langle K_1^c, K_1^c\\rangle_F \\; \\langle K_2^c, K_2^c \\rangle_F}}\n",
    "\\end{equation*}\n",
    "\n",
    "with $K^c$ is the centered version of $K$ and can be calculated as\n",
    "\n",
    "\\begin{equation*}\n",
    "K^c = K - \\frac{1}{N} 11^TK - \\frac{1}{N} K11^T + \\frac{1}{N^2} \\left(1^TK1\\right)11^T\n",
    "\\end{equation*}\n",
    "\n",
    "where $1$ is the vector of ones with proper dimension.\n",
    "\n",
    "The optimization problem \\eqref{eq:problem} has then a unique analytical solution\n",
    "\n",
    "\\begin{equation}\n",
    "\\eta = \\frac{M^{-1}a}{||M^{-1}a||_2}\n",
    "\\label{eq:problem_solution}\n",
    "\\end{equation}\n",
    "\n",
    "where $M = \\{\\langle K_m, K_h\\rangle_F\\}_{m,h \\, = \\, 1}^P$ and $a = \\{\\langle K_m, IK\\rangle_F\\}_{m \\, = \\, 1}^P$.\n",
    "\n",
    "In order to avoid overfitting, to be more robust in the learning procedure and in order to get deeper insights of the problem we added either an $L_1$ or $L_2$ penalty to the functional \\eqref{eq:problem}, obtaining the following two reformulations\n",
    "\n",
    "\\begin{equation}\n",
    "\\max\\limits_{\\eta \\in \\mathcal{M}} CA(K_{\\eta}, IK) - \\lambda ||\\eta||_2^2 \\;\\; (a), \\qquad \\max\\limits_{\\eta \\in \\mathcal{M}} CA(K_{\\eta}, IK) - c ||\\eta||_1 \\;\\; (b).\n",
    "\\label{eq:problem_penalty}\n",
    "\\end{equation}\n",
    "\n",
    "While the Equation \\eqref{eq:problem_penalty}(a) has the unique analytical solution \n",
    "\n",
    "\\begin{equation}\n",
    "\\eta = \\frac{(M-\\lambda I)^{-1}a}{||(M-\\lambda I)^{-1}a||_2}\n",
    "\\label{eq:problem_solution_lambda}\n",
    "\\end{equation}\n",
    "\n",
    "Equation \\eqref{eq:problem_penalty}(b) does not due to the non-differentiability of the $L_1$ norm. To converge to one of the optima we applied the forward-backward splitting method [2][3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The Algorithm\n",
    "\n",
    "<ol>\n",
    "    <li><i>Datasets loading</i> $\\rightarrow$ (Clinical, Genetic, Vampire, Outputs);</li>\n",
    "    <li><i>Outer split</i> $\\rightarrow$ 75% training + 25% test for final testing purposes;</li>\n",
    "    <li><i>Kernel definition</i> $\\rightarrow$ dictionary list: $[\\{\\;<kernelType1>:[parameter list],\\;<kernelType2>:[parameter list],\\;\\cdots\\;\\},\\;\\cdots\\;]$</li>\n",
    "    <li><i>Sampling rounds</i> $\\rightarrow$ 3 rounds of 75% training + 25% mid test (extracted from the previous 75% of training) for statistical stability:\n",
    "        <ol type=\"i\">\n",
    "            <li></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "(the sampling results will be processed in order to find the best configuration over one dictionary):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Approach Setting\n",
    "\n",
    "Kernel used:\n",
    "    \n",
    "<ul>\n",
    "    <li>Gaussian: $\\hspace{4mm}K_{\\sigma}(x_i, x) = e^{\\,-\\,\\left(\\frac{||x_i \\, - \\, x||^2}{2\\sigma}\\right)}$</li>\n",
    "    <li>Linear: $\\hspace{9mm}K(x_i, x) = x_i \\cdot x^T$</li>\n",
    "    <li>Polynomial: $\\hspace{1mm}K_d(x_i, x) = (1 +  x_i \\cdot x^t)^d$</li>\n",
    "    <li>Laplacian: $\\hspace{3mm}K_{\\gamma}(x_i, x) = e^{\\,-\\,(\\, \\gamma \\, ||x_i \\, - \\, x||_1)}$</li>\n",
    "    <li>Sigmoid: $\\hspace{5mm}K_{\\gamma}(x_i, x) = \\tanh \\,(\\,\\gamma \\, \\langle X, Y \\rangle + 1)$</li>\n",
    "</ul>\n",
    "\n",
    "The basical approach exploited has been picking three different kernels and applying them to every dataset, getting back 9 kernel matrices. Several combinations of three data preprocessing methods (Data origin centering, Data Normalization, Kernel Normalization) have been tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Toy Testing\n",
    "\n",
    "In order to analyze the correctness of the algorithm implemented three synthetic datasets have been generated and the algorithm have been launched over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Toy Dataset Generation\n",
    "\n",
    "For dataset generation we used the sklearn.datasets.make_classification function [4], initialized with the following configuration:\n",
    "\n",
    "<ul>\n",
    "    <li>n_samples = 300</li>\n",
    "    <li>n_features = 30</li>\n",
    "    <li>n_informative = 10</li>\n",
    "    <li>n_redundant = 0</li>\n",
    "    <li>n_classes = 2</li>\n",
    "</ul>\n",
    "\n",
    "The data gathered in this way have been splitted in three datasets and the informative variables have been distributed evenly between the first two. This procedure let us to check if the algorithm is able or not to percieve the irrelevance of the third dataset.\n",
    "\n",
    "Then, the three datasets have been splitted in training and test sets exploiting sklearn.model_selection.StratifiedShuffleSplit [5]. The initialization in the next\n",
    "\n",
    "<ul>\n",
    "    <li>n_splits = 1</li>\n",
    "    <li>test_size = 0.25</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Corinna Cortes, Mehryar Mohri, and Rostamizadeh Afshin. \"Two-stage learning kernel algorithms\", In Proceedings of the 27th International Conference on Machine Learning.<br>\n",
    "[2] https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning<br>\n",
    "[3] Combettes, Patrick L.; Wajs, Valérie R. (2005). \"Signal Recovering by Proximal Forward-Backward Splitting\", Multiscale Model, Simul 4 (4): 1168–1200. doi:10.1137/050626090<br>\n",
    "[4] http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html<br>\n",
    "[5] http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
